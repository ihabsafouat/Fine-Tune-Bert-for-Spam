{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abe47eca-c545-4054-8739-5aaad687af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c241db8a-13e3-48ea-99fb-7216d0050a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf53b7e-3861-4a6f-8dd8-c4811758f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/Ihab/Desktop/PCyber/spam.csv')\n",
    "texts = data['Message'].tolist()\n",
    "labels = data['Category'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e83d95d-aca3-46d1-b1b8-19b8c135d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = {'ham': 1, 'spam': 0}\n",
    "data['Category'] = data['Category'].map(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5041c5a4-c28f-4538-ad05-4a344e9e5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['Message'].tolist()\n",
    "labels = data['Category'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1547910d-af32-4de0-be81-7212c7491940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225bd3a27d2e46fb88aacff39811c16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ihab\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ihab\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd6f1629b5e405bb583e01027f7ed76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5f6c7fcb234bd7934500f924aff3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1c4c895ce841569bf1b830b6b45160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d76198e7-60a5-4998-a508-98faa1f8eadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20df742d-12a0-468f-b6c6-b20d98b30315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1/5\n",
      "Training fold 2/5\n",
      "Training fold 3/5\n",
      "Training fold 4/5\n",
      "Training fold 5/5\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(kf.split(texts)):\n",
    "    print(f\"Training fold {fold+1}/{kf.get_n_splits()}\")\n",
    "    \n",
    "    # Split data for the current fold\n",
    "    X_train, X_val = [texts[i] for i in train_idx], [texts[i] for i in val_idx]\n",
    "    y_train, y_val = [labels[i] for i in train_idx], [labels[i] for i in val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21b4a872-7a6d-4dce-b0db-de1cf1616bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(\n",
    "        X_train,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "val_encodings = tokenizer(\n",
    "        X_val,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef537baf-7a8c-4796-b3c2-3516f7608b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be40bffc8b404119ae961bac2c334352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model.compile(optimizer=Adam(learning_rate=2e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a33b80c-8bef-4eaa-a4e1-0b40b7378a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train))\n",
    "        .shuffle(1000)\n",
    "        .batch(16)\n",
    "        .cache()\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "val_dataset = (\n",
    "        tf.data.Dataset.from_tensor_slices((dict(val_encodings), y_val))\n",
    "        .batch(16)\n",
    "        .cache()\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ccc43d-1464-45d5-9de8-602768b8cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, validation_data=val_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd83467-c169-409c-a031-0a2829b1e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "\n",
    "\n",
    "    val_loss, val_accuracy = model.evaluate(val_dataset)\n",
    "    accuracies.append(val_accuracy)\n",
    "\n",
    "print(f\"Mean Accuracy after K-Folds: {sum(accuracies)/len(accuracies)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
